在概率論和統計學中，相關（Correlation），顯示兩個隨機變量之間線性關係的強度和方向。在統計學中，相關的意義是用來衡量兩個變量相對於其相互獨立的距離。在這個廣義的定義下，有許多根據數據特點而定義的用來衡量數據相關的係數。

英國生物學家和統計學家弗朗西斯·高爾頓首先提出「相關」這一概念，英國數學家卡爾·皮爾遜在此基礎上做出了進一步發展。

對於不同測量尺度的變數，有不同的相關係數可用：

其中，E是數學期望，cov表示共變異數，




σ

X




{\displaystyle \sigma _{X}}

和




σ

Y




{\displaystyle \sigma _{Y}}

是標準差。

因為




μ

X


=
E
(
X
)


{\displaystyle \mu _{X}=E(X)}

，




σ

X


2


=
E
(

X

2


)
−

E

2


(
X
)


{\displaystyle \sigma _{X}^{2}=E(X^{2})-E^{2}(X)}

，同樣地，對於



Y


{\displaystyle Y}

，可以寫成

當兩個變量的標準差都不為零，相關係數才有定義。從柯西-施瓦茨不等式可知，相關係數的絕對值不超過1。當兩個變量的線性關係增強時，相關係數趨於1或-1。當一個變量增加而另一變量也增加時，相關係數大於0。當一個變量的增加而另一變量減少時，相關係數小於0。當兩個變量獨立時，相關係數為0，但反之並不成立。這是因為相關係數僅僅反映了兩個變量之間是否線性相關。比如說，X是區間［－1，1］上的一個均勻分布的隨機變量。Y = X2.那麼Y是完全由X確定。因此Y和X不獨立，但相關係數為0。或者說他們是不相關的。當Y和X服從聯合常態分布時，其相互獨立和不相關是等價的。

當一個或兩個變量帶有測量誤差時，他們的相關性就受到削弱，這時，「反衰減」性（disattenuation）是一個更準確的係數。

對於居中的數據來說（何謂居中？也就是每個數據減去樣本均值，居中後它們的平均值就為0），相關係數可以看作是兩個隨機變量中得到的樣本集向量之間夾角的cosine函數。一些實際工作者更喜歡用非居中的相關係數（與皮爾遜係數不相兼容）。看下面的例子中有一個比較。例如，假設五個國家的國民生產總值分別是1、2、3、5、8（單位10億美元），又假設這五個國家的貧困比例分別是11%、12%、13%、15%、18%。則我們現在有兩個有序的包含5個元素的向量x、y：x =（1, 2, 3, 5, 8）、 y =（0.11, 0.12, 0.13, 0.15, 0.18）
使用一般的方法來計算向量間夾角（參考數量積），未居中的相關性係數如下：

上面的數據實際上是故意選擇了一個完美的線性關係：y = 0.10 + 0.01 x。因此皮爾遜相關係數應該就是1。把數據居中（x中數據減去E (x) = 3.8，y中數據減去E (y) = 0.138）後得到：x =（−2.8, −1.8, −0.8, 1.2, 4.2）、y =（−0.028, −0.018, −0.008, 0.012, 0.042），由此得到了預期結果：

相關係數的計算過程可表示為：將每個變量都轉化為標準單位，乘積的平均數即為相關係數[1]。

兩個變量的關係可以直觀地用散點圖表示，當其緊密地群聚於一條直線的周圍時，變量間存在強相關[2]。

一個散點圖可以用五個統計量來概括。所有x值得平均數，所有x值的SD，所有y值得平均數，所有y值的SD，相關係數r.

將第一個變量記為x ,第二個變量記為y ,相關係數為r，則可以通過以下公式：

r = [（以標準單位表示的x）X（以標準單位表示的y）]的平均數

